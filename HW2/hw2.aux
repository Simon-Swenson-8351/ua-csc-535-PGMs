\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Q1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}a}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}b}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}c}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}d}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}e}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}f}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}g}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}h}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Q2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Q3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}a}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Q3.a: Three Gaussian density functions with different means and standard deviations. The means define the centers of the humps. In addition, since these are probability density functions, the area under each one is one. A consequence of this is that when the standard deviation is smaller, the top of the curve gets higher. In other words, the narrower the hump of the curve is, the taller it must go to maintain an integral of one.}}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}b}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Q3.b: A 3D Gaussian distribution with a mean of [0, 0] and a covariance matrix of [0.5 0.3; 0.3 2.0]. Notice how the values in the covariance matrix affect the graph. In particular, the value at the bottom left of the covariance matrix is much higher than the other values. This affects the graph in the y direction, causing it to be much more gradual than the graph in the x direction. You can almost think of the diagonals of the covariance matrix as the variance in that graph's unit vector (x, y, etc.) direction. However, when multiple features (axes) are taken together, they can vary with each other, as well, hence we have a matrix instead of a vector.}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Q4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Q4: The estimation of $ p(x) $ by marginalizing $ y $. The graph appears to be another Gaussian distribution. This is not unexpected, but it is good to see that Gaussian distributions are well-behaved in this way.}}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Q5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Q5: The estimation of $ p(y | x = 0.5) $. Given the output matrix of $ Z $ used to graph Q3.b, we can simply grab the column where $ x = 0.5 $ to become the new output values of our conditioned function. However, to ensure p sums to one, we must norm those values. Similar to Q4, the output is another Gaussian distribution. Thus, we see that both marginalization and conditioning a multivariate Gaussian distribution leads to another Gaussian distribtution.}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Q6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}a}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Proving B from A}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Proving C from A}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Proving A from C}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Q7}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}a}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}b}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}c}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}d}{8}}
